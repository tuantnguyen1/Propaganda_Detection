{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from preprocess_c import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "train_path  = 'data/data_propoganda/data/protechn_corpus_eval/train'\n",
    "test_path = 'data/data_propoganda/data/protechn_corpus_eval/test'\n",
    "dev_path = 'data/data_propoganda/data/protechn_corpus_eval/dev'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dset(path):\n",
    "    path_ = Path(path)\n",
    "    a = make_dataset(path_)\n",
    "    df_1 = pd.DataFrame(columns=['id','full_sent','start_sent','end_sent','start_prop','end_prop','prop','??','???'])\n",
    "    for dm in a:\n",
    "        df_t = pd.DataFrame(dm,columns =['id','full_sent','start_sent','end_sent','start_prop','end_prop','prop','??','???'] )\n",
    "        df_1 = df_1.append(df_t,ignore_index= True)\n",
    "    return df_1.iloc[:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = make_dset(train_path)\n",
    "#df_train= pd.read_csv('Context_only (1).csv')\n",
    "df_test = make_dset(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin = [0 if i == 'O' else 1 for i in df_train.prop.values ]\n",
    "df_train['binary'] = bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapping = {'Loaded_Language':1,'Name_Calling,Labeling':2,'Repetition':3,\n",
    "           'Exaggeration,Minimisation':4,'Doubt':5,'Appeal_to_fear-prejudice':6,'Flag-Waving':7,'Causal_Oversimplification':8,\n",
    "           'Slogans':9,'Appeal_to_Authority':10,'Black-and-White_Fallacy':11,'Thought-terminating_Cliches':12,'Whataboutism':13,\n",
    "           'Reductio_ad_hitlerum':14,'Red_Herring':15,'Bandwagon':16,'Obfuscation,Intentional_Vagueness,Confusion':17,'Straw_Men':18,'O':0}\n",
    "#df_train = df_train[df_train.binary !=0]\n",
    "\n",
    "df_train['prop_1'] = df_train.prop.apply(lambda x: mapping[x])\n",
    "df_test['prop_1'] = df_test.prop.apply(lambda x: mapping[x])\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "train_direct = glob.glob('data/data_propoganda/data/protechn_corpus_eval/train/*.txt')\n",
    "articles = []\n",
    "def read_articles():\n",
    "  for filename in train_direct:\n",
    "    myfile = open(filename)\n",
    "    article = myfile.read()\n",
    "    articles.append(article)\n",
    "    myfile.close()\n",
    "  article_ids = []\n",
    "  for filename in train_direct:\n",
    "    article_ids.append(filename[60:-4])\n",
    "  \n",
    "  return articles, article_ids\n",
    "articles,art_ids = read_articles()\n",
    "id2art ={i:a for a,i in zip(articles,art_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(article, span, mode='sentence'):\n",
    "  article = id2art[article]\n",
    "  def get_num_words(sentence):\n",
    "    return len(sentence.split(' '))\n",
    "  if mode == \"title\":\n",
    "    return article.split('\\n')[0]\n",
    "  if mode == \"sentence\":\n",
    "    WORD_LEN_LIMIT = 120\n",
    "    li = span[0]\n",
    "    ri = span[1]\n",
    "    span_text = article[li: ri]\n",
    "    num_words = get_num_words(span_text)\n",
    "    if num_words >= WORD_LEN_LIMIT:\n",
    "      return span_text\n",
    "    remaining_len = WORD_LEN_LIMIT - num_words\n",
    "    lhs_words = remaining_len // 2\n",
    "    rhs_words = remaining_len - lhs_words\n",
    "    li -= 1\n",
    "    lcount = 0\n",
    "    while li >= 0 and article[li-1] != '\\n' and lcount < lhs_words:\n",
    "      if article[li] == ' ':\n",
    "        lcount += 1\n",
    "      li -= 1\n",
    "    ri += 1\n",
    "    rcount = 0\n",
    "    while ri < len(article) and article[ri] != '\\n' and rcount < rhs_words:\n",
    "      if article[ri] == ' ':\n",
    "        rcount += 1\n",
    "      ri += 1\n",
    "    return article[li+1: ri - 1] \n",
    "\n",
    "  return \"\"\n",
    "spans_1 = [(i,k,j) for i,k,j in zip(df_train.id,df_train.start_sent,df_train.end_sent)]\n",
    "df_train['context'] = [get_context(i,(s,e)) for i,s,e in spans_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.full_sent[index])\n",
    "        title = \" \".join(title.split())\n",
    "        context = str(self.data.context[index])\n",
    "        context = \" \".join(context.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        tokenized_context = self.tokenizer.encode_plus(context,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=self.max_len,\n",
    "                                            pad_to_max_length=True,\n",
    "                                            return_attention_mask=True,truncation = True)\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        c_ids = tokenized_context['input_ids']\n",
    "        c_mask = tokenized_context['attention_mask']\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.prop_1[index], dtype=torch.long),\n",
    "            'c_ids':torch.tensor(c_ids, dtype=torch.long),\n",
    "            'c_mask': torch.tensor(c_mask,dtype= torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (15750, 10)\n",
      "TRAIN Dataset: (12600, 10)\n",
      "TEST Dataset: (3150, 10)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset=df_train.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df_train.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df_train.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss, MSELoss,BCEWithLogitsLoss\n",
    "\n",
    "class ContextualBertForSequenceClassification(torch.nn.Module):\n",
    "  \n",
    "  def __init__(self, num_labels, ContextModel, SpanModel):\n",
    "    super(ContextualBertForSequenceClassification, self).__init__()\n",
    "    self.ContextModel = ContextModel\n",
    "    self.SpanModel = SpanModel\n",
    "    self.num_labels = num_labels\n",
    "\n",
    "    # self.classifier = torch.nn.Linear(768*2, num_labels)\n",
    "    # self.classifier1 = torch.nn.Linear(768, num_labels)\n",
    "    self.classifier2 = torch.nn.Linear(768+128, num_labels)\n",
    "    self.reduce_classifier = torch.nn.Linear(768, 128)\n",
    "    self.dropout = torch.nn.Dropout(0.1)\n",
    "\n",
    "  def forward(\n",
    "      self,\n",
    "      span_input_ids,\n",
    "      span_attention_mask,\n",
    "      context_input_ids,\n",
    "      context_attention_mask,\n",
    "      labels=None\n",
    "  ):\n",
    "    context_outputs = self.ContextModel(\n",
    "        input_ids=context_input_ids,\n",
    "        attention_mask=context_attention_mask\n",
    "    )\n",
    "    context_outputs = context_outputs[1] # pooler output\n",
    "    span_outputs = self.SpanModel(\n",
    "        input_ids=span_input_ids,\n",
    "        attention_mask=span_attention_mask\n",
    "    )\n",
    "    span_outputs = span_outputs[1]\n",
    "\n",
    "    context_outputs = self.reduce_classifier(context_outputs)\n",
    "    pooled_output = torch.cat((span_outputs, context_outputs), axis=1)\n",
    "\n",
    "    pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "    logits = self.classifier2(pooled_output)\n",
    "    outputs = (logits,)\n",
    "    if labels is not None:\n",
    "      if self.num_labels == 1:\n",
    "        loss_fct = MSELoss()\n",
    "        loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "      else:\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "      outputs = (loss,) + outputs\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "  labels_flat = labels.flatten()\n",
    "  return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=5):\n",
    "  loss_values = []\n",
    "  for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(training_loader):\n",
    "      if step % 100 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(training_loader), elapsed))\n",
    "      b_input_ids = batch['ids'].to(device)\n",
    "      b_labels = batch['targets'].to(device, dtype = torch.long)\n",
    "      b_input_mask = batch['mask'].to(device)\n",
    "      b_c_input_ids = batch['c_ids'].to(device)\n",
    "      b_c_input_mask = batch['c_mask'].to(device)\n",
    "      model.zero_grad()        \n",
    "      outputs = model(b_input_ids, \n",
    "                      b_input_mask,\n",
    "                      b_c_input_ids, \n",
    "                      b_c_input_mask, \n",
    "                      labels=b_labels)\n",
    "      loss = outputs[0]\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "      optimizer.step()\n",
    "      scheduler.step() # TODO\n",
    "      stats = '[%d/%d][%d/%d]\\tLoss: %.4f' \\\n",
    "                % (epoch_i+1, epochs, step, len(training_loader), loss.item())\n",
    "      print('\\r' + stats, end=\"\")\n",
    "      sys.stdout.flush()\n",
    "    avg_train_loss = total_loss / len(training_loader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in testing_loader:\n",
    "      # batch = tuple(t.to(device) for t in batch)\n",
    "      b_input_ids = batch['ids'].to(device)\n",
    "      b_labels = batch['targets'].to(device, dtype = torch.long)\n",
    "      b_input_mask = batch['mask'].to(device)\n",
    "      b_c_input_ids = batch['c_ids'].to(device)\n",
    "      b_c_input_mask = batch['c_mask'].to(device)\n",
    "      with torch.no_grad():        \n",
    "        outputs = model(b_input_ids, \n",
    "                        b_input_mask,\n",
    "                        b_c_input_ids, \n",
    "                        b_c_input_mask)\n",
    "      logits = outputs[0]\n",
    "      logits = logits.detach().cpu().numpy()\n",
    "      label_ids = b_labels.to('cpu').numpy()\n",
    "      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "      eval_accuracy += tmp_eval_accuracy\n",
    "      nb_eval_steps += 1\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "  print(\"\")\n",
    "  print(\"Training complete!\")\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(model, dataloader):\n",
    "  model.eval()\n",
    "  predictions , true_labels = [], []\n",
    "  nb_eval_steps = 0\n",
    "  for batch in dataloader:\n",
    "    b_input_ids = batch['ids'].to(device)\n",
    "    b_labels = batch['targets'].to(device)\n",
    "    b_input_mask = batch['mask'].to(device)\n",
    "    b_c_input_ids = batch['c_ids'].to(device)\n",
    "    b_c_input_mask = batch['c_mask'].to(device)\n",
    "    with torch.no_grad():        \n",
    "      logits = model(b_input_ids, \n",
    "                     b_input_mask,\n",
    "                     b_c_input_ids, \n",
    "                     b_c_input_mask)\n",
    "    \n",
    "    logits = logits[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    pred_label = np.argmax(logits, axis=1) #[1 if a >0.5 else 0 for a in logits]\n",
    "    predictions.extend(pred_label)\n",
    "    true_labels.extend(label_ids)\n",
    "  return predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dev_predictions(model):\n",
    "  test_articles, _ = read_articles(\"dev-articles\")\n",
    "  test_spans, test_techniques = read_test_spans()\n",
    "\n",
    "  test_articles = test_articles[1:]\n",
    "  test_dataloader = get_data(test_articles, test_spans, test_techniques)\n",
    "  pred, _ = get_model_predictions(model, test_dataloader)\n",
    "\n",
    "  with open('predictions.txt', 'w') as fp:\n",
    "    label_file = os.path.join(data_dir, \"dev-task-TC-template.out\")\n",
    "    myfile = open(label_file)\n",
    "    prev_index = -1\n",
    "    tsvreader = csv.reader(myfile, delimiter=\"\\t\")\n",
    "    for i, row in enumerate(tsvreader):\n",
    "      fp.write(row[0] + '\\t' + distinct_techniques[pred[i]] + '\\t' + row[2] + '\\t' + row[3] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "#from transformers import RobertaModel\n",
    "from transformers import BertModel\n",
    "#from transformers import RobertaForSequenceClassification\n",
    "import time,sys\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "context_model = BertModel.from_pretrained(model_name)\n",
    "span_model = BertModel.from_pretrained(model_name)\n",
    "model = ContextualBertForSequenceClassification(19, context_model, span_model)\n",
    "model.cuda()\n",
    "\"\"\"model.load_state_dict(torch.load('best.pth'))\n",
    "model.eval()\"\"\"\n",
    "optimizer = AdamW(model.parameters(),lr = 3e-5,eps = 1e-8) # ler = 5e-5\n",
    "epochs = 7\n",
    "\n",
    "total_steps = len(training_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "train(model, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in model if saved. ('best.pth' is the model name)\n",
    "\"\"\"model.load_state_dict(torch.load('best.pth'))\n",
    "model.eval()\"\"\"\n",
    "\n",
    "#predict and get classification report\n",
    "\"\"\"pred,true = get_model_predictions(model,testing_loader)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(pred,true))\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "concacenvi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
